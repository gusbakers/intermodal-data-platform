{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Data Quality Report\n",
    "\n",
    "**Purpose:** Comprehensive data quality assessment using industry standards.\n",
    "\n",
    "**Quality Dimensions:**\n",
    "1. **Completeness** - Are all required fields populated?\n",
    "2. **Accuracy** - Are values within expected ranges?\n",
    "3. **Consistency** - Are values consistent across records?\n",
    "4. **Timeliness** - Is the data current?\n",
    "5. **Validity** - Do values conform to business rules?\n",
    "\n",
    "*This demonstrates understanding of data governance principles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet('../data/processed/shipments_processed.parquet')\n",
    "print(f\"‚úÖ Loaded {len(df):,} records for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Completeness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate completeness metrics\n",
    "completeness = {}\n",
    "for col in df.columns:\n",
    "    non_null = df[col].notna().sum()\n",
    "    total = len(df)\n",
    "    completeness[col] = (non_null / total * 100)\n",
    "\n",
    "completeness_df = pd.DataFrame.from_dict(completeness, orient='index', columns=['Completeness %'])\n",
    "completeness_df = completeness_df.sort_values('Completeness %')\n",
    "\n",
    "# Visualize\n",
    "fig = px.bar(\n",
    "    completeness_df,\n",
    "    y=completeness_df.index,\n",
    "    x='Completeness %',\n",
    "    orientation='h',\n",
    "    title='üìä Data Completeness by Column',\n",
    "    color='Completeness %',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    range_color=[0, 100]\n",
    ")\n",
    "\n",
    "fig.add_vline(x=95, line_dash=\"dash\", line_color=\"red\", annotation_text=\"95% Threshold\")\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "# Quality score\n",
    "avg_completeness = completeness_df['Completeness %'].mean()\n",
    "print(f\"\\nüìà Overall Completeness Score: {avg_completeness:.2f}%\")\n",
    "\n",
    "if avg_completeness >= 95:\n",
    "    print(\"‚úÖ EXCELLENT - Data meets quality standards\")\n",
    "elif avg_completeness >= 85:\n",
    "    print(\"‚ö†Ô∏è GOOD - Minor improvements needed\")\n",
    "else:\n",
    "    print(\"‚ùå POOR - Significant data quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Accuracy Check (Value Ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected ranges (business rules)\n",
    "validation_rules = {\n",
    "    'cost': {'min': 0, 'max': 100000, 'type': 'numeric'},\n",
    "    'weight': {'min': 0, 'max': 100000, 'type': 'numeric'},\n",
    "    'transit_days': {'min': 0, 'max': 365, 'type': 'numeric'} if 'transit_days' in df.columns else None\n",
    "}\n",
    "\n",
    "# Check violations\n",
    "violations = {}\n",
    "for col, rules in validation_rules.items():\n",
    "    if rules and col in df.columns:\n",
    "        below_min = (df[col] < rules['min']).sum()\n",
    "        above_max = (df[col] > rules['max']).sum()\n",
    "        violations[col] = {'below_min': below_min, 'above_max': above_max}\n",
    "\n",
    "# Display violations\n",
    "violations_df = pd.DataFrame(violations).T\n",
    "print(\"üîç Range Validation Results:\")\n",
    "print(violations_df)\n",
    "\n",
    "total_violations = violations_df.sum().sum()\n",
    "if total_violations == 0:\n",
    "    print(\"\\n‚úÖ No range violations detected\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {int(total_violations)} range violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logical consistency\n",
    "consistency_issues = []\n",
    "\n",
    "# Rule 1: Arrival date should be after shipment date\n",
    "if 'shipment_date' in df.columns and 'arrival_date' in df.columns:\n",
    "    invalid_dates = df[df['arrival_date'] < df['shipment_date']]\n",
    "    if len(invalid_dates) > 0:\n",
    "        consistency_issues.append(f\"‚ö†Ô∏è {len(invalid_dates)} records with arrival before shipment\")\n",
    "\n",
    "# Rule 2: Cost per kg should be positive\n",
    "if 'cost_per_kg' in df.columns:\n",
    "    invalid_cost = df[df['cost_per_kg'] <= 0]\n",
    "    if len(invalid_cost) > 0:\n",
    "        consistency_issues.append(f\"‚ö†Ô∏è {len(invalid_cost)} records with invalid cost per kg\")\n",
    "\n",
    "# Rule 3: Transit days should match date difference\n",
    "if all(col in df.columns for col in ['shipment_date', 'arrival_date', 'transit_days']):\n",
    "    df['calculated_transit'] = (df['arrival_date'] - df['shipment_date']).dt.days\n",
    "    mismatched = df[df['transit_days'] != df['calculated_transit']]\n",
    "    if len(mismatched) > 0:\n",
    "        consistency_issues.append(f\"‚ö†Ô∏è {len(mismatched)} records with mismatched transit days\")\n",
    "\n",
    "# Display results\n",
    "print(\"üîç Consistency Check Results:\")\n",
    "print(\"=\" * 60)\n",
    "if consistency_issues:\n",
    "    for issue in consistency_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"‚úÖ No consistency issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Uniqueness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate IDs\n",
    "if 'shipment_id' in df.columns:\n",
    "    duplicates = df['shipment_id'].duplicated().sum()\n",
    "    unique_pct = (1 - duplicates / len(df)) * 100\n",
    "    \n",
    "    print(f\"üîë Shipment ID Uniqueness: {unique_pct:.2f}%\")\n",
    "    \n",
    "    if duplicates == 0:\n",
    "        print(\"‚úÖ All shipment IDs are unique\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Found {duplicates} duplicate shipment IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Overall Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall quality score\n",
    "scores = {\n",
    "    'Completeness': avg_completeness,\n",
    "    'Uniqueness': unique_pct if 'shipment_id' in df.columns else 100,\n",
    "    'Accuracy': 100 - (total_violations / len(df) * 100) if total_violations > 0 else 100,\n",
    "    'Consistency': 100 - (len(consistency_issues) * 5)  # -5 points per issue type\n",
    "}\n",
    "\n",
    "# Ensure scores are between 0-100\n",
    "scores = {k: max(0, min(100, v)) for k, v in scores.items()}\n",
    "\n",
    "overall_score = sum(scores.values()) / len(scores)\n",
    "\n",
    "# Visualize quality dimensions\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatterpolar(\n",
    "        r=list(scores.values()),\n",
    "        theta=list(scores.keys()),\n",
    "        fill='toself',\n",
    "        name='Quality Score'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(visible=True, range=[0, 100])\n",
    "    ),\n",
    "    title='üìä Data Quality Dashboard',\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Final report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "for dimension, score in scores.items():\n",
    "    print(f\"{dimension:15s}: {score:6.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OVERALL SCORE: {overall_score:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if overall_score >= 95:\n",
    "    print(\"\\n‚úÖ EXCELLENT - Production ready\")\n",
    "elif overall_score >= 85:\n",
    "    print(\"\\n‚ö†Ô∏è GOOD - Minor improvements recommended\")\n",
    "elif overall_score >= 70:\n",
    "    print(\"\\n‚ö†Ô∏è FAIR - Improvements needed before production\")\n",
    "else:\n",
    "    print(\"\\n‚ùå POOR - Significant remediation required\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
